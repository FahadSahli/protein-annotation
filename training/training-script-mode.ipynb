{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# To ensure TF runs eagerly\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# S3 client to read files stored in S3\n",
    "client = boto3.client(\"s3\")\n",
    "\n",
    "# A class that defines the model\n",
    "class ProtCNN(tf.keras.Model):\n",
    "    def __init__(self, unique_classes):\n",
    "        \n",
    "        super(ProtCNN, self).__init__()\n",
    "        self.unique_classes = unique_classes\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(32, 1, strides=1, padding='valid', name='conv1d_1', \n",
    "                   kernel_initializer=tf.keras.initializers.glorot_uniform(seed=0))\n",
    "        self.max_pool_1 = layers.MaxPool1D(pool_size=2)\n",
    "        self.batch_norm_1 = layers.BatchNormalization(axis=2, name='batch_norm_1')\n",
    "        \n",
    "        self.activation_1 = layers.Activation('relu', name='activation_1')\n",
    "        self.batch_norm_2 = layers.BatchNormalization(axis=2, name='batch_norm_2')\n",
    "        self.activation_2 = layers.Activation('relu', name='activation_2')\n",
    "        \n",
    "        \n",
    "        self.conv1d_2 = layers.Conv1D(32, 1, strides=1, padding='valid', name='conv1d_2', \n",
    "                           kernel_initializer=tf.keras.initializers.glorot_uniform(seed=0))\n",
    "        self.batch_norm_3 = layers.BatchNormalization(axis=2, name='batch_norm_3')\n",
    "        self.activation_3 = layers.Activation('relu', name='activation_3')\n",
    "        \n",
    "        \n",
    "        self.conv1d_3 = layers.Conv1D(32, 1, strides=1, padding='valid', name='conv1d_3',\n",
    "                                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=0))\n",
    "        self.dropout_1 = layers.Dropout(0.5, name='dropout_1')\n",
    "        self.max_pool_2 = layers.MaxPool1D(pool_size=2)\n",
    "        \n",
    "        \n",
    "        self.conv1d_4 = layers.Conv1D(32,  1, strides=1, padding='valid', name='conv1d_4',\n",
    "                                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=0))\n",
    "        self.dropout_2 = layers.Dropout(0.5, name='dropout_2')\n",
    "        self.max_pool_3 = layers.MaxPool1D(pool_size=2)\n",
    "        \n",
    "        \n",
    "        self.added = layers.Add()\n",
    "        \n",
    "        self.activation_4 = layers.Activation('relu',name='activation_4')\n",
    "        self.dropout_3 = layers.Dropout(0.2, name='dropout_3')\n",
    "        \n",
    "        self.batch_norm_4 = layers.BatchNormalization(axis=2, name='batch_norm_4')\n",
    "        self.activation_5 = layers.Activation('relu', name='activation_5')\n",
    "        self.dropout_4 = layers.Dropout(0.5, name='dropout_4')\n",
    "        \n",
    "        self.flatten = layers.Flatten(name='flatten')\n",
    "        self.dense = layers.Dense(self.unique_classes, name=\"dense\",\n",
    "                          kernel_initializer=tf.keras.initializers.glorot_uniform(seed=0))\n",
    "        self.final_output = layers.Activation('softmax', name='output')\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        This method is called whenever the model is invoked, either for training or testing.\n",
    "        x is the input, and it has a shape of (batch, max_len, amino_acids).\n",
    "        \"\"\"\n",
    "        conv1d_1 = self.conv1d_1(x)\n",
    "        max_pool_1 = self.max_pool_1(conv1d_1)\n",
    "        batch_norm_1 = self.batch_norm_1(max_pool_1)\n",
    "\n",
    "        activation_1 = self.activation_1(batch_norm_1)\n",
    "        batch_norm_2 = self.batch_norm_2(activation_1)\n",
    "        activation_2 = self.activation_2(batch_norm_2)\n",
    "\n",
    "        conv1d_2 = self.conv1d_2(activation_2)\n",
    "        batch_norm_3 = self.batch_norm_3(conv1d_2)\n",
    "        activation_3 = self.activation_3(batch_norm_3)\n",
    "\n",
    "        conv1d_3 = self.conv1d_3(activation_3)\n",
    "        dropout_1 = self.dropout_1(conv1d_3)\n",
    "        max_pool_2 = self.max_pool_2(dropout_1)\n",
    "\n",
    "        conv1d_4 = self.conv1d_4(activation_1)\n",
    "        dropout_2 = self.dropout_2(conv1d_4)\n",
    "        max_pool_3 = self.max_pool_3(dropout_2)\n",
    "\n",
    "        added = self.added([max_pool_2, max_pool_3])\n",
    "\n",
    "        activation_4 = self.activation_4(added)\n",
    "        dropout_3 = self.dropout_3(activation_4)\n",
    "\n",
    "        batch_norm_4 = self.batch_norm_4(dropout_3)\n",
    "        activation_5 = self.activation_5(batch_norm_4)\n",
    "        dropout_4 = self.dropout_4(activation_5)\n",
    "\n",
    "        flatten = self.flatten(dropout_4)\n",
    "        dense = self.dense(flatten)\n",
    "        output = self.final_output(dense)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def inp_df_to_np(inp_df):\n",
    "    \"\"\"\n",
    "    This method converts input sequences into numpy arrays.\n",
    "    \n",
    "    Each entry in inp_df dataframe is a string of the form \"[7, 10, 5, 1, 19, 14, 10, 10, 3, ...]\",\n",
    "        where the numbers act as IDs for the 25 letters that makeup a sequence. The brackets are part\n",
    "        of the string and should be removed.\n",
    "        \n",
    "    The method returns an array with shape (batch, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the first entry in inp_df, remove brackets, and split letters\n",
    "    splited = inp_df.iloc[0].strip(\"[]\").split(\", \")\n",
    "    \n",
    "    # Convert the list of IDs to numpy array of integers\n",
    "    array = np.asarray([int(x) for x in splited], dtype=np.intc)\n",
    "    \n",
    "    # Pad the array with zeros\n",
    "    array = tf.keras.preprocessing.sequence.pad_sequences([array], maxlen=max_len, dtype='int32', padding='post', value=0)\n",
    "    \n",
    "    # Loop over the rest of the entries in inp_df\n",
    "    for i in range(1, len(inp_df)):\n",
    "        splited = inp_df.iloc[i].strip(\"[]\").split(\", \")\n",
    "        temp = np.asarray([int(x) for x in splited], dtype=np.intc)\n",
    "        temp = tf.keras.preprocessing.sequence.pad_sequences([temp], maxlen=max_len, dtype='int32', padding='post', value=0)\n",
    "        array = np.concatenate((array, temp), axis=0) \n",
    "    \n",
    "    return array\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    \"\"\"\n",
    "    This method passes data to the model, computes loss, and updates model parameters.\n",
    "    Inputes:\n",
    "        1. inp, a batch of training sequences.\n",
    "        2. targ, a batch of target labels.\n",
    "        \n",
    "    Outputs:\n",
    "        1. batch_loss.\n",
    "        2. batch_accuracy.\n",
    "    \"\"\"\n",
    "    global prot_cnn, optimizer, loss_object\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # A Keras object to calculate accuracy\n",
    "    batch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        enc_output = prot_cnn(inp, training=True)\n",
    "        \n",
    "        # Compute and aggregate loss\n",
    "        loss += loss_function(targ, enc_output)\n",
    "        \n",
    "    # Compute average loss\n",
    "    batch_loss = (loss / int(targ.shape[0]))\n",
    "    \n",
    "    # Get trainable variables, gradients, and update model parameters\n",
    "    variables = prot_cnn.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Compute accuracy\n",
    "    batch_accuracy.update_state(targ, enc_output)\n",
    "    \n",
    "    # batch_loss is a tf scaler value, we need to convert it to numpy object\n",
    "    # Tha same applies to batch_accuracy.result()\n",
    "    return batch_loss.numpy(), batch_accuracy.result().numpy()\n",
    "\n",
    "def run_validation(X_val, Y_val, batch=1024):\n",
    "    \"\"\"\n",
    "    This method computes loss and accuracy on validation or test data.\n",
    "    \n",
    "    Outputs:\n",
    "        1. Average loss.\n",
    "        2. Average accuracy.\n",
    "        3. Average processing time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # end_time is a variable to calculate the time needed to process data\n",
    "    end_time = 0\n",
    "    \n",
    "    loss = 0\n",
    "    validation_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    for i in range(0, len(X_val), batch):\n",
    "        print(\"Working on batch {}\".format(i//batch))\n",
    "        \n",
    "        # Pre-process sequences\n",
    "        x_val = inp_df_to_np(X_val[i:i+batch])\n",
    "        x_val = tf.keras.utils.to_categorical(x_val , amino_acids)\n",
    "\n",
    "        # Pre-process labels\n",
    "        y_val = tf.keras.utils.to_categorical(Y_val[i:i+batch] , unique_classes)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        validation_results = prot_cnn(x_val, training=False)\n",
    "        end_time = (time.time() - start_time) + end_time\n",
    "\n",
    "        loss += loss_function(y_val, validation_results)\n",
    "        validation_accuracy.update_state(y_val, validation_results)\n",
    "        \n",
    "    return (loss / (i//batch + 1)).numpy(), validation_accuracy.result().numpy(), end_time\n",
    "    \n",
    "def main(args):\n",
    "    global unique_classes, amino_acids, max_len, prot_cnn, optimizer, loss_object\n",
    "    \n",
    "    unique_classes = args.unique_classes\n",
    "    amino_acids = args.amino_acids\n",
    "    max_len = args.max_len\n",
    "    \n",
    "    print(\"Building model...\")\n",
    "    prot_cnn = ProtCNN(unique_classes)\n",
    "    prot_cnn.build((None, max_len, amino_acids))\n",
    "    \n",
    "    print(\"Defining optimizer and loss function\")\n",
    "    optimizer_type = args.optimizer_type\n",
    "    if (optimizer_type == \"sgd\"):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=args.learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate, beta_1=0.997, clipvalue=1.0)\n",
    "    \n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Data directories\n",
    "    train_dir = \"/opt/ml/input/data/train\"\n",
    "    validation_dir = \"/opt/ml/input/data/validation\"\n",
    "    test_dir = \"/opt/ml/input/data/test\"\n",
    "\n",
    "    print(\"Defining checkpoint\")\n",
    "    # Define a checkpoint to save model while and after training\n",
    "    checkpoint_dir = \"/opt/ml/model\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(prot_cnn=prot_cnn)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=1)\n",
    "    status = checkpoint.restore(manager.latest_checkpoint)\n",
    "    \n",
    "    # A dataframe to store history of training\n",
    "    history = pd.DataFrame(columns=['train_loss', 'train_accuracy', 'validation_loss', 'validation_accuracy'])\n",
    "    \n",
    "    print(\"Starting training\")\n",
    "    \n",
    "    # Get validation data\n",
    "    all_validation_files = os.listdir(validation_dir)\n",
    "    # delete follwing 2\n",
    "    print(\"all_validation_files\")\n",
    "    print(all_validation_files)\n",
    "    validation_generator = (pd.read_csv(os.path.join(validation_dir, f)) for f in all_validation_files)\n",
    "    validation_data = pd.concat(validation_generator, ignore_index=True)\n",
    "    X_validation = validation_data[\"sequence\"]\n",
    "    Y_validation = validation_data[\"family_accession\"]\n",
    "    \n",
    "    # Training data is splitted into 80 files, but we will read 10 files at a time\n",
    "    partitions = [i for i in range(len(os.listdir(train_dir))//10)]\n",
    "    \n",
    "    EPOCHS = args.epochs\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Recored start time of an epoch\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        train_loss = 0\n",
    "        batch = args.batch_size\n",
    "        train_acc = 0\n",
    "        \n",
    "        # total_batch keeps track the number of batchs accross the entire training data\n",
    "        total_batch = 0\n",
    "        \n",
    "        # Loop over training partitions (files)\n",
    "        for partition in partitions:\n",
    "            print(\"Training on partition {}...\".format(partition))\n",
    "            \n",
    "            ten_train_files = os.listdir(train_dir)[partition*10:partition*10+10]\n",
    "            train_generator = (pd.read_csv(os.path.join(train_dir, f)) for f in ten_train_files)\n",
    "            train_data = pd.concat(train_generator, ignore_index=True)\n",
    "            X_train = train_data[\"sequence\"]\n",
    "            Y_train = train_data[\"family_accession\"]\n",
    "\n",
    "            for i in range(0, len(X_train), batch):\n",
    "                # Pre-process sequences\n",
    "                x_train = inp_df_to_np(X_train[i:i+batch])\n",
    "                x_train = tf.keras.utils.to_categorical(x_train , amino_acids)\n",
    "\n",
    "                # Pre-process labels\n",
    "                y_train = tf.keras.utils.to_categorical(Y_train[i:i+batch] , unique_classes)\n",
    "\n",
    "                batch_loss, batch_acc = train_step(x_train, y_train)\n",
    "                train_loss += batch_loss\n",
    "                train_acc += batch_acc\n",
    "\n",
    "                # Delete training objects to save memory\n",
    "                del x_train\n",
    "                del y_train\n",
    "\n",
    "                if ((i//batch + total_batch) % 100 == 0):\n",
    "                    print(\"Done with batch {} of epoch {}...\".format(i//batch + total_batch, epoch+1))\n",
    "                    print(\"Loss: {:.10f}, accuracy: {:.10f}\".format(train_loss / (i//batch + total_batch + 1),\n",
    "                                                                    train_acc / (i//batch + total_batch + 1)))\n",
    "            # End of one partition\n",
    "            \n",
    "            total_batch += (i//batch + 1)\n",
    "            \n",
    "            # Perform a validation over a random batch from validation data\n",
    "            random_batch = random.randint(0, len(X_validation)-batch)\n",
    "            _, validation_acc, _ = run_validation(X_validation[random_batch:random_batch+batch],\n",
    "                                                  Y_validation[random_batch:random_batch+batch])\n",
    "            \n",
    "            print(\"Random validation accuracy is {}\".format(validation_acc))\n",
    "            \n",
    "        # End of an epoch\n",
    "        \n",
    "        # Save the progress so far\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        train_loss = train_loss / total_batch\n",
    "        train_acc = train_acc / total_batch\n",
    "\n",
    "        print('Epoch {}, train loss {:.10f}, train accuracy {:.10f}'.format(epoch + 1, train_loss, train_acc))\n",
    "        print('Time taken for an epoch is {} sec\\n'.format(time.time() - epoch_start))\n",
    "\n",
    "        # Perform validation\n",
    "        print(\"Validating model...\")\n",
    "        validation_loss, validation_acc, validation_time = run_validation(X_validation, Y_validation)\n",
    "\n",
    "        print(\"Validation loss is {}\".format(validation_loss))\n",
    "        print(\"Validation accuracy is {}\".format(validation_acc))\n",
    "        print(\"The model processes {} samples per second\\n\".format(round(len(X_validation)/validation_time, 4)))\n",
    "        \n",
    "        # Recored training history\n",
    "        history_dic = {\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'validation_loss': validation_loss,\n",
    "            'validation_accuracy': validation_acc\n",
    "        }\n",
    "        \n",
    "        # Update history dataframe\n",
    "        history = history.append(history_dic, ignore_index=True)\n",
    "        \n",
    "    # End of training\n",
    "    \n",
    "    # Write history dataframe to S3\n",
    "    csv_buffer = StringIO()\n",
    "    history.to_csv(csv_buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, 'history.csv').put(Body=csv_buffer.getvalue())\n",
    "        \n",
    "    # Perform testing...\n",
    "    print(\"Testing model...\")\n",
    "    all_test_files = os.listdir(test_dir)\n",
    "    test_generator = (pd.read_csv(os.path.join(test_dir, f)) for f in all_test_files)\n",
    "    test_data = pd.concat(test_generator, ignore_index=True)\n",
    "    X_test = test_data[\"sequence\"]\n",
    "    Y_test = test_data[\"family_accession\"]\n",
    "\n",
    "    test_loss, test_acc, test_time = run_validation(X_test, Y_test)\n",
    "\n",
    "    print(\"Test loss is {}\".format(test_loss))\n",
    "    print(\"Test accuracy is {}\".format(test_acc))\n",
    "    print(\"The model processes {} samples per second\".format(round(len(X_test)/test_time, 4)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    This method is the entry point of the program\n",
    "    \"\"\"\n",
    "    \n",
    "    global bucket\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "    parser.add_argument('--model_dir', type=str, required=True, help='The directory where the model will be stored.')\n",
    "    parser.add_argument('--bucket', type=str, required=True, help='SageMaker default bucket.')\n",
    "    \n",
    "    # Training hyper-parameters\n",
    "    parser.add_argument('--epochs', type=int, default=5)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--optimizer_type', type=str, default=\"adam\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    \n",
    "    # Model hyper-parameters\n",
    "    \"\"\"\n",
    "    The parameters are:\n",
    "        1. unique_classes is the total number of classes to be predicted.\n",
    "        2. max_len is the maximun length a sequence can have. Shorter sequences are padded with zeros.\n",
    "        3. 25 amino_acids (indexed from 1) plus 1 to account for zero padding. So it defaults to 26 (i.e., 25 + 1).  \n",
    "    \"\"\"\n",
    "    parser.add_argument('--unique_classes', type=int, default=17929)\n",
    "    parser.add_argument('--max_len', type=int, default=2037)\n",
    "    parser.add_argument('--amino_acids', type=int, default=26)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    bucket = args.bucket\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ps_instance_type = 'ml.p3.2xlarge'\n",
    "ps_instance_count = 1\n",
    "\n",
    "model_dir = 's3://' + os.path.join(bucket, 'model_artifacts')\n",
    "\n",
    "hyperparameters = {\"bucket\":bucket , 'epochs': 10, 'batch_size' : 512, 'learning_rate': 0.001}\n",
    "\n",
    "estimator_ps = TensorFlow(\n",
    "                        entry_point='train.py', \n",
    "                        base_job_name='training-protCNN',\n",
    "                        role=role,\n",
    "                        framework_version=\"2.1.0\",\n",
    "                        script_mode=True,\n",
    "                        py_version= \"py3\",\n",
    "                        image_name='763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-gpu-py37-cu102-ubuntu18.04',\n",
    "                        hyperparameters=hyperparameters,\n",
    "                        instance_count=ps_instance_count, \n",
    "                        instance_type=ps_instance_type,\n",
    "                        model_dir=model_dir,\n",
    "                        volume_size = 70\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator_ps.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ns3_data = {\\n    \"train\": \"s3://<bucket>/train\",\\n    \"test\": \"s3://<bucket>/test\",\\n    \"validation\": \"s3://<bucket>/validation\"\\n}\\n\\nestimator_ps.fit(s3_data)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "s3_data = {\n",
    "    \"train\": \"s3://<bucket>/train\",\n",
    "    \"test\": \"s3://<bucket>/test\",\n",
    "    \"validation\": \"s3://<bucket>/validation\"\n",
    "}\n",
    "\n",
    "estimator_ps.fit(s3_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, CategoricalParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'optimizer_type': CategoricalParameter(['sgd', 'adam']),\n",
    "    'learning_rate': CategoricalParameter([0.001, 0.01])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'test-accuracy'\n",
    "metric_definitions = [\n",
    "    {\n",
    "        'Name': 'test-accuracy',\n",
    "        'Regex': 'Test accuracy is ([0-9\\\\.]+)'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'Name': 'test-loss',\n",
    "        'Regex': 'Test loss is ([0-9\\\\.]+)'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'Name': 'train-accuracy',\n",
    "        'Regex': 'train accuracy ([0-9\\\\.]+)'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'Name': 'train-loss',\n",
    "        'Regex': 'train loss ([0-9\\\\.]+)'\n",
    "    },\n",
    "    \n",
    "    \n",
    "    {\n",
    "        'Name': 'validation-accuracy',\n",
    "        'Regex': 'Validation accuracy is ([0-9\\\\.]+)'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'Name': 'validation-loss',\n",
    "        'Regex': 'Validation loss is ([0-9\\\\.]+)'\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator_ps,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=4,\n",
    "                            max_parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-033c3cce3813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_tuning_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_with_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3181\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhyperparameter\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3182\u001b[0m         \"\"\"\n\u001b[0;32m-> 3183\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tuning_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperParameterTuningJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   4427\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4428\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4429\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4430\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_data_bucket = \"cloud-formation-processed-data\"\n",
    "\n",
    "s3_data = {\n",
    "    \"train\": os.path.join(\"s3://\", processed_data_bucket, \"train\"),\n",
    "    \"test\": os.path.join(\"s3://\", processed_data_bucket, \"test\"),\n",
    "    \"validation\": os.path.join(\"s3://\", processed_data_bucket, \"validation\")\n",
    "}\n",
    "\n",
    "tuner.fit(s3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
