{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation with Amazon SageMaker Processing and Dask\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Dask are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Dask in a managed SageMaker environment to run our preprocessing workload.\n",
    "\n",
    "### What is Dask Distributed?\n",
    "Dask.distributed: is a lightweight and open source library for distributed computing in Python. It is also a centrally managed, distributed, dynamic task scheduler. It is also a centrally managed, distributed, dynamic task scheduler. Dask has three main components:\n",
    "\n",
    "**dask-scheduler process:** coordinates the actions of several workers. The scheduler is asynchronous and event-driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers.\n",
    "\n",
    "**dask-worker processes:** Which are spread across multiple machines and the concurrent requests of several clients.\n",
    "\n",
    "**dask-client process:** which is is the primary entry point for users of dask.distributed\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-overview.svg\">\n",
    "\n",
    "source: https://docs.dask.org/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective:-predict-the-age-of-an-Abalone-from-its-physical-measurement)\n",
    "1. [Setup](#Setup)\n",
    "1. [Using Amazon SageMaker Processing to execute a Dask Job](#Using-Amazon-SageMaker-Processing-to-execute-a-Dask-Job)\n",
    "  1. [Downloading dataset and uploading to S3](#Downloading-dataset-and-uploading-to-S3)\n",
    "  1. [Build a Dask container for running the preprocessing job](#Build-a-Dask-container-for-running-the-preprocessing-job)\n",
    "  1. [Run the preprocessing job using Amazon SageMaker Processing](#Run-the-preprocessing-job-using-Amazon-SageMaker-Processing)\n",
    "    1. [Inspect the preprocessed dataset](#Inspect-the-preprocessed-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-877465308896'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = \"validation\"\n",
    "input_prefix = 'row-data/' + data_subset + \".csv\"\n",
    "input_preprocessed_prefix = 'processed-data/' + data_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker Processing to execute a Dask job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset and uploading to Amazon Simple Storage Service (Amazon S3)\n",
    "\n",
    "The dataset used here is the Census-Income KDD Dataset. The first step are to select features, clean the data, and turn the data into features that the training algorithm can use to train a binary classification model which can then be used to predict whether rows representing census responders have an income greater or less than $50,000. In this example, we will use Dask distributed to preprocess and transform the data to make it ready for the training process. In the next section, you download from the bucket below then upload to your own bucket so that Amazon SageMaker can access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dask container for running the preprocessing job\n",
    "\n",
    "An example Dask container is included in the `./container` directory of this example. The container handles the bootstrapping of Dask Scheduler and mapping each instance to a Dask Worke. At a high level the container provides:\n",
    "\n",
    "* A set of default worker/scheduler configurations\n",
    "* A bootstrapping script for configuring and starting up  scheduler/worker nodes\n",
    "* Starting dask cluster from all the workers including the scheduler node\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed dask application that performs our dataset preprocessing.\n",
    "\n",
    "### Build the example Dask container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/amazon-sagemaker-examples/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask/container\n",
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/21 : FROM continuumio/miniconda3:4.7.12\n",
      " ---> 406f2b43ea59\n",
      "Step 2/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 6ba1dd8a9b44\n",
      "Step 3/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> cdf2de405d5e\n",
      "Step 4/21 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> b3ff4e4ec52f\n",
      "Step 5/21 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 9568dd651e28\n",
      "Step 6/21 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> b7939b0f11ab\n",
      "Step 7/21 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 8e369638938c\n",
      "Step 8/21 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> ab317a7a48ee\n",
      "Step 9/21 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> b698a6a930b3\n",
      "Step 10/21 : RUN conda install --yes     -c conda-forge     python==3.8     python-blosc     cytoolz     dask==2.16.0     distributed==2.16.0     lz4     nomkl     numpy==1.18.1     pandas==1.0.1     tini==0.18.0     && conda clean -tipsy     && find /opt/conda/ -type f,l -name '*.a' -delete     && find /opt/conda/ -type f,l -name '*.pyc' -delete     && find /opt/conda/ -type f,l -name '*.js.map' -delete     && find /opt/conda/lib/python*/site-packages/bokeh/server/static -type f,l -name '*.js' -not -name '*.min.js' -delete     && rm -rf /opt/conda/pkgs\n",
      " ---> Using cache\n",
      " ---> 04217d5776ae\n",
      "Step 11/21 : RUN pip install dask-ml\n",
      " ---> Using cache\n",
      " ---> 529a46c59167\n",
      "Step 12/21 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Using cache\n",
      " ---> 98d3f583f536\n",
      "Step 13/21 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Using cache\n",
      " ---> f494783ddbef\n",
      "Step 14/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> a8efe2fe4af8\n",
      "Step 15/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> 3931459d3e63\n",
      "Step 16/21 : RUN conda install --yes s3fs -c conda-forge\n",
      " ---> Using cache\n",
      " ---> a45c36f09c39\n",
      "Step 17/21 : RUN mkdir /opt/app /etc/dask\n",
      " ---> Using cache\n",
      " ---> 846c2406a997\n",
      "Step 18/21 : COPY dask_config/dask.yaml /etc/dask/\n",
      " ---> Using cache\n",
      " ---> 0ed61c48b8cf\n",
      "Step 19/21 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> a0e8bdfe2ba7\n",
      "Step 20/21 : RUN chmod +x /opt/program/bootstrap.py\n",
      " ---> Using cache\n",
      " ---> 0c5e5446cbd5\n",
      "Step 21/21 : ENTRYPOINT [\"/opt/program/bootstrap.py\"]\n",
      " ---> Using cache\n",
      " ---> 1330aa0da4ad\n",
      "Successfully built 1330aa0da4ad\n",
      "Successfully tagged sagemaker-dask-example:latest\n",
      "/home/ec2-user/SageMaker/amazon-sagemaker-examples/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask\n"
     ]
    }
   ],
   "source": [
    "%cd container\n",
    "!docker build -t sagemaker-dask-example .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Dask container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'sagemaker-dask-example'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "dask_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $dask_repository_uri\n",
    "!docker push $dask_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing job using Amazon SageMaker Processing on Dask Cluster\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the the custom Dask container that was just built, and a Scikit Learn script for preprocessing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dask preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from __future__ import print_function, unicode_literals\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tornado import gen\n",
    "import dask.dataframe as dd\n",
    "import joblib\n",
    "from dask.distributed import Client\n",
    "\n",
    "df_freq = pd.read_csv('s3://sagemaker-us-east-1-877465308896/freq_df.csv')\n",
    "\n",
    "def split_sequence(sequence):\n",
    "    '''\n",
    "    Separate cahrs in sequence with spaces: 'ABCDEFG' to ' A B C D E F G'\n",
    "    '''\n",
    "    \n",
    "    separated_seq = ''\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        separated_seq = separated_seq + ' ' + sequence[i]\n",
    "        \n",
    "    return separated_seq\n",
    "\n",
    "def sequence_to_ID(sequence):\n",
    "    \"\"\"\n",
    "    Replace chars in a sequence with its ID\n",
    "    \"\"\"\n",
    "    return list(map(char_to_ID, sequence.split()))\n",
    "\n",
    "def char_to_ID(char):\n",
    "    return df_freq.index[df_freq['feature'] == char.lower()].tolist()[0] + 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Get processor scrip arguments\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    script_args = dict(zip(args_iter, args_iter))\n",
    "    scheduler_ip = sys.argv[-1]\n",
    "\n",
    "    # S3 client\n",
    "    s3_region = script_args[\"s3_region\"]\n",
    "    print(f'Using the {s3_region} region')\n",
    "    \n",
    "    # Start the Dask cluster client\n",
    "    try:\n",
    "        client = Client(\"tcp://{ip}:8786\".format(ip=scheduler_ip))\n",
    "        logging.info(\"Printing cluster information: {}\".format(client))\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "    \n",
    "    input_data_path = \"s3://{}\".format(os.path.join(script_args[\"s3_input_bucket\"], script_args[\"s3_input_key_prefix\"]))\n",
    "    output_data_path = \"s3://{}\".format(os.path.join(script_args[\"s3_output_bucket\"], script_args[\"s3_output_key_prefix\"]))\n",
    "\n",
    "    logging.info(\"Reading input data from {}\".format(input_data_path))\n",
    "    df_row_data = pd.read_csv(input_data_path)\n",
    "\n",
    "    # Read dictionary mapping family accessions to their values\n",
    "    dict_class = pd.read_csv(\"s3://{}\".format(os.path.join(script_args[\"s3_input_bucket\"], 'dict_class.csv')))\n",
    "    dict_class = dict_class.to_dict(\"dict\")\n",
    "    \n",
    "    logging.info(\"Running preprocessing and feature engineering transformations in Dask\")\n",
    "    with joblib.parallel_backend(\"dask\"):\n",
    "        \n",
    "        logging.info(\"Working with sequences\")\n",
    "        \n",
    "        # Perform spacing between the chars of sequences\n",
    "        x_splited = (df_row_data.sequence).apply(split_sequence)\n",
    "        \n",
    "        # Convert chars to their IDs\n",
    "        x_splited_IDs = x_splited.apply(sequence_to_ID)\n",
    "        \n",
    "        \n",
    "        logging.info(\"Working with labels\")\n",
    "\n",
    "        # Construct a DF of labels (e.g., true values)\n",
    "        labels = df_row_data.family_accession.apply(lambda x: dict_class[x][0])\n",
    "\n",
    "    logging.info(\"Done, writing data to: {}\".format(output_data_path))\n",
    "    pd.DataFrame(x_splited_IDs).to_csv(output_data_path + \"_x.csv\", index=False, mode = 'w', header = True)\n",
    "    pd.DataFrame(labels).to_csv(output_data_path + \"_y.csv\", index=False, mode = 'w', header = True)\n",
    "\n",
    "    \n",
    "    print(client)\n",
    "    sys.exit(os.EX_OK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a processing job using the Docker image and preprocessing script you just created. When invoking the `dask_processor.run()` function, pass the Amazon S3 input and output paths as arguments that are required by our preprocessing script to determine input and output location in Amazon S3. Here, you also specify the number of instances and instance type that will be used for the distributed Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ScriptProcessor\n",
    "\n",
    "dask_processor = ScriptProcessor(\n",
    "    base_job_name=\"dask-preprocessor\",\n",
    "    image_uri=dask_repository_uri,\n",
    "    command=[\"/opt/program/bootstrap.py\"],\n",
    "    role=role,\n",
    "    instance_count=3,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  dask-preprocessor-2020-10-12-19-39-11-931\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-877465308896/dask-preprocessor-2020-10-12-19-39-11-931/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      ".......................\u001b[32mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.131.225:46767'\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Start worker at:   tcp://10.0.131.225:34079\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          Listening to:   tcp://10.0.131.225:34079\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          dashboard at:         10.0.131.225:36079\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Waiting to connect to:     tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -                Memory:                   14.99 GB\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-lbxyav6c\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -         Registered to:     tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[35mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.173.153:34415'\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Start worker at:   tcp://10.0.173.153:44363\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          Listening to:   tcp://10.0.173.153:44363\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          dashboard at:         10.0.173.153:39879\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Waiting to connect to:     tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -                Memory:                   14.99 GB\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-46opuc95\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -         Registered to:     tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.172.50:34277'\u001b[0m\n",
      "\u001b[34mdistributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Clear task state\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   Scheduler at:    tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Start worker at:    tcp://10.0.172.50:40271\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          Listening to:    tcp://10.0.172.50:40271\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          dashboard at:          10.0.172.50:43725\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - Waiting to connect to:     tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -                Memory:                   14.99 GB\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-kh_1n8_p\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.172.50:40271', name: tcp://10.0.172.50:40271, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.172.50:40271\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -         Registered to:     tcp://10.0.172.50:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.131.225:34079', name: tcp://10.0.131.225:34079, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.131.225:34079\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.173.153:44363', name: tcp://10.0.173.153:44363, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.173.153:44363\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-246fe5a6-0cc3-11eb-8020-ae8b0c135aa3\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mUsing the us-east-1 region\u001b[0m\n",
      "\u001b[34m<Client: 'tcp://10.0.172.50:8786' processes=3 threads=12, memory=44.96 GB>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-246fe5a6-0cc3-11eb-8020-ae8b0c135aa3\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-246fe5a6-0cc3-11eb-8020-ae8b0c135aa3\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Close client connection: Client-246fe5a6-0cc3-11eb-8020-ae8b0c135aa3\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[32mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[35mReceived a shutdown signal from Dask cluster\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dask_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    arguments=[\n",
    "        \"s3_input_bucket\",\n",
    "        bucket,\n",
    "        \"s3_input_key_prefix\",\n",
    "        input_prefix,\n",
    "        \"s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"s3_output_key_prefix\",\n",
    "        input_preprocessed_prefix,\n",
    "        \"s3_region\",\n",
    "        region\n",
    "    ],\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
